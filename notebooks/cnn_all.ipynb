{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN for many subjects\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero:\n",
    "\n",
    "Cargamos los datos y los normalizamos. Para esto, primero pasamos un filtro pasa-bajo de 0 a 20hz, luego lo normalizamos a $N(0, 1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: TkAgg\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jmperez/.pyenv/versions/3.6.5/envs/p300/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU's disponibles = ['/job:localhost/replica:0/task:0/device:GPU:0']\n"
     ]
    }
   ],
   "source": [
    "%pylab\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "if \"..\" not in sys.path:\n",
    "    sys.path.append(\"..\")\n",
    "import glob\n",
    "import os\n",
    "import mne\n",
    "from keras import backend as K\n",
    "from p300.preprocessing import normalize_subject, load_data\n",
    "\n",
    "print(\"GPU's disponibles = {}\".format(K.tensorflow_backend._get_available_gpus()))\n",
    "\n",
    "CORPORA_PATH = \"~/projects/corpora/P3Speller/P3Speller-old-y-datos/sets\"\n",
    "\n",
    "file_path = os.path.expanduser(CORPORA_PATH)\n",
    "files = glob.glob(os.path.join(file_path, \"*.set\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Targets appear as 2 in the third column\n",
    "\n",
    "\n",
    "We remove last channel as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# this line is to avoid output\n",
    "pretraining_no = 100\n",
    "\n",
    "training_files = files[:pretraining_no]\n",
    "testing_files = files[pretraining_no:]\n",
    "\n",
    "X_train, y_train = load_data(training_files)\n",
    "# Check that there are no overlaps!\n",
    "assert(len([f for f in training_files if f in testing_files]) == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {0: 0.6, 1: 3.0}\n",
      "(198360, 14, 104, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "y_t = y_train.reshape(-1)\n",
    "class_weights = class_weight.compute_class_weight('balanced', np.unique(y_t), y_t)\n",
    "\n",
    "class_weights = dict(zip([0,1], class_weights))\n",
    "\n",
    "print(\"Class weights: {}\".format(class_weights))\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jmperez/.pyenv/versions/3.6.5/envs/p300/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1264: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/jmperez/.pyenv/versions/3.6.5/envs/p300/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1349: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, Conv2D, Flatten, Dense, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "activation = 'relu'\n",
    "\n",
    "n_kernels = 12\n",
    "model.add(Conv2D(n_kernels, (14, 1), padding='same',\n",
    "                activation=activation, input_shape=(14, 104, 1)))\n",
    "model.add(Conv2D(5*n_kernels, (1, 13), padding='same',\n",
    "                activation=activation))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.45))\n",
    "model.add(Dense(128, activation=activation))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', # using the cross-entropy loss function\n",
    "              optimizer='rmsprop', \n",
    "              metrics=['accuracy']) # reporting the accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 178524 samples, validate on 19836 samples\n",
      "Epoch 1/40\n",
      "177920/178524 [============================>.] - ETA: 0s - loss: 0.6903 - acc: 0.5737Epoch 00001: val_loss improved from inf to 0.65889, saving model to models/model_cnn_1.h5\n",
      "178524/178524 [==============================] - 19s 105us/step - loss: 0.6903 - acc: 0.5735 - val_loss: 0.6589 - val_acc: 0.6389\n",
      "Epoch 2/40\n",
      "178432/178524 [============================>.] - ETA: 0s - loss: 0.6742 - acc: 0.6203Epoch 00002: val_loss did not improve\n",
      "178524/178524 [==============================] - 18s 98us/step - loss: 0.6742 - acc: 0.6203 - val_loss: 0.6664 - val_acc: 0.7408\n",
      "Epoch 3/40\n",
      "178176/178524 [============================>.] - ETA: 0s - loss: 0.6709 - acc: 0.6372Epoch 00003: val_loss improved from 0.65889 to 0.65717, saving model to models/model_cnn_1.h5\n",
      "178524/178524 [==============================] - 18s 99us/step - loss: 0.6710 - acc: 0.6372 - val_loss: 0.6572 - val_acc: 0.5029\n",
      "Epoch 4/40\n",
      "177920/178524 [============================>.] - ETA: 0s - loss: 0.6680 - acc: 0.6413Epoch 00004: val_loss improved from 0.65717 to 0.65303, saving model to models/model_cnn_1.h5\n",
      "178524/178524 [==============================] - 18s 99us/step - loss: 0.6680 - acc: 0.6411 - val_loss: 0.6530 - val_acc: 0.6195\n",
      "Epoch 5/40\n",
      "177920/178524 [============================>.] - ETA: 0s - loss: 0.6663 - acc: 0.6469Epoch 00005: val_loss improved from 0.65303 to 0.65203, saving model to models/model_cnn_1.h5\n",
      "178524/178524 [==============================] - 18s 99us/step - loss: 0.6664 - acc: 0.6470 - val_loss: 0.6520 - val_acc: 0.5901\n",
      "Epoch 6/40\n",
      "178176/178524 [============================>.] - ETA: 0s - loss: 0.6640 - acc: 0.6547Epoch 00006: val_loss improved from 0.65203 to 0.64760, saving model to models/model_cnn_1.h5\n",
      "178524/178524 [==============================] - 18s 99us/step - loss: 0.6639 - acc: 0.6548 - val_loss: 0.6476 - val_acc: 0.7129\n",
      "Epoch 7/40\n",
      "178176/178524 [============================>.] - ETA: 0s - loss: 0.6625 - acc: 0.6595Epoch 00007: val_loss did not improve\n",
      "178524/178524 [==============================] - 18s 98us/step - loss: 0.6625 - acc: 0.6596 - val_loss: 0.6540 - val_acc: 0.7205\n",
      "Epoch 8/40\n",
      "178432/178524 [============================>.] - ETA: 0s - loss: 0.6612 - acc: 0.6613Epoch 00008: val_loss did not improve\n",
      "178524/178524 [==============================] - 18s 98us/step - loss: 0.6612 - acc: 0.6614 - val_loss: 0.6522 - val_acc: 0.7484\n",
      "Epoch 9/40\n",
      "178176/178524 [============================>.] - ETA: 0s - loss: 0.6609 - acc: 0.6640Epoch 00009: val_loss did not improve\n",
      "178524/178524 [==============================] - 18s 99us/step - loss: 0.6610 - acc: 0.6640 - val_loss: 0.6520 - val_acc: 0.6462\n",
      "Epoch 10/40\n",
      "178176/178524 [============================>.] - ETA: 0s - loss: 0.6597 - acc: 0.6633Epoch 00010: val_loss did not improve\n",
      "178524/178524 [==============================] - 18s 99us/step - loss: 0.6596 - acc: 0.6633 - val_loss: 0.6514 - val_acc: 0.7291\n",
      "Epoch 11/40\n",
      "178432/178524 [============================>.] - ETA: 0s - loss: 0.6586 - acc: 0.6655Epoch 00011: val_loss did not improve\n",
      "178524/178524 [==============================] - 18s 99us/step - loss: 0.6585 - acc: 0.6656 - val_loss: 0.6536 - val_acc: 0.7482\n",
      "CPU times: user 2min 41s, sys: 34.7 s, total: 3min 15s\n",
      "Wall time: 3min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "checkpointer = ModelCheckpoint(filepath='models/model_cnn_1.h5', verbose=1, save_best_only=True)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train, epochs=40, \n",
    "    batch_size=256, class_weight=class_weights, validation_split=0.10,\n",
    "    callbacks=[checkpointer, early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fix the first four layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<keras.layers.convolutional.Conv2D at 0x7f8fc9599a20>, 'Trainable: False'),\n",
       " (<keras.layers.convolutional.Conv2D at 0x7f8fc95999b0>, 'Trainable: False'),\n",
       " (<keras.layers.core.Flatten at 0x7f8fc9599cf8>, 'Trainable: False'),\n",
       " (<keras.layers.core.Dropout at 0x7f8fc955fe80>, 'Trainable: False'),\n",
       " (<keras.layers.core.Dense at 0x7f8fc955fe48>, 'Trainable: True'),\n",
       " (<keras.layers.core.Dense at 0x7f8fc954b320>, 'Trainable: True')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def fix_layers(model, fixed_layers):\n",
    "    for i in range(fixed_layers):\n",
    "        model.layers[i].trainable = False\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop', \n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "fix_layers(model, 4)    \n",
    "\n",
    "[(l, \"Trainable: {}\".format(l.trainable)) for l in model.layers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the idea is to train each subject and fine tune the last layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture output\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import (\n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    roc_auc_score, \n",
    "    accuracy_score, \n",
    "    f1_score\n",
    ")\n",
    "from p300.preprocessing import normalize_subject, load_data, load_data_from_subject\n",
    "\n",
    "file = files[130]\n",
    "\n",
    "def get_fine_tune_results(model_path, file):\n",
    "    K.clear_session()\n",
    "    \n",
    "    model = load_model(model_path)\n",
    "    \n",
    "    fix_layers(model, 4)\n",
    "\n",
    "    X_sub, y_sub = load_data([file])\n",
    "\n",
    "    length = X_sub.shape[0] \n",
    "    limit = int(length / 2)\n",
    "    X_sub_train, X_sub_test = X_sub[:limit], X_sub[limit:]\n",
    "    y_sub_train, y_sub_test = y_sub[:limit], y_sub[limit:]\n",
    "    \n",
    "    model.fit(\n",
    "        X_sub_train, y_sub_train, epochs=20, \n",
    "        batch_size=64, class_weight=class_weights, validation_split=0.01,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    \n",
    "    y_pred = model.predict_classes(X_sub_test)\n",
    "    y_prob = model.predict(X_sub_test)\n",
    "\n",
    "    precision = precision_score(y_sub_test, y_pred)\n",
    "    recall = recall_score(y_sub_test, y_pred)\n",
    "    auc = roc_auc_score(y_sub_test, y_prob)\n",
    "    accuracy = accuracy_score(y_sub_test, y_pred)\n",
    "    f1 = f1_score(y_sub_test, y_pred)\n",
    "    \n",
    "    subject_name = file.split(\"/\")[-1].split(\".\")[0].split(\"_\")[-1]\n",
    "    K.clear_session()\n",
    "    return {\n",
    "        \"subject\": subject_name,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1\": f1\n",
    "    }, model\n",
    "    \n",
    "\n",
    "\n",
    "model_path = 'models/model_cnn_1.h5'\n",
    "\n",
    "all_results = []\n",
    "for file in testing_files:\n",
    "    try:\n",
    "        all_results.append(get_fine_tune_results(model_path, file)[0])\n",
    "    except Exception as e:\n",
    "        # if file is not ok, discard\n",
    "        print(\"=\"*80)\n",
    "        print(\"=\"*80)\n",
    "        print(e)\n",
    "        print(\"=\"*80)\n",
    "        print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10924001</th>\n",
       "      <td>0.746825</td>\n",
       "      <td>0.405959</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.519048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22650001</th>\n",
       "      <td>0.589683</td>\n",
       "      <td>0.262482</td>\n",
       "      <td>0.187373</td>\n",
       "      <td>0.438095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11693001</th>\n",
       "      <td>0.764646</td>\n",
       "      <td>0.371968</td>\n",
       "      <td>0.334951</td>\n",
       "      <td>0.418182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26715001</th>\n",
       "      <td>0.677778</td>\n",
       "      <td>0.372294</td>\n",
       "      <td>0.275641</td>\n",
       "      <td>0.573333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22109001</th>\n",
       "      <td>0.715556</td>\n",
       "      <td>0.396226</td>\n",
       "      <td>0.306569</td>\n",
       "      <td>0.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31397001</th>\n",
       "      <td>0.753535</td>\n",
       "      <td>0.290698</td>\n",
       "      <td>0.279330</td>\n",
       "      <td>0.303030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20668001</th>\n",
       "      <td>0.764444</td>\n",
       "      <td>0.442105</td>\n",
       "      <td>0.365217</td>\n",
       "      <td>0.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29426001</th>\n",
       "      <td>0.663636</td>\n",
       "      <td>0.335329</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.509091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29257001</th>\n",
       "      <td>0.747778</td>\n",
       "      <td>0.349570</td>\n",
       "      <td>0.306533</td>\n",
       "      <td>0.406667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10229001</th>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.299754</td>\n",
       "      <td>0.237354</td>\n",
       "      <td>0.406667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4824001</th>\n",
       "      <td>0.571111</td>\n",
       "      <td>0.215447</td>\n",
       "      <td>0.154971</td>\n",
       "      <td>0.353333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23794001</th>\n",
       "      <td>0.688095</td>\n",
       "      <td>0.369181</td>\n",
       "      <td>0.278450</td>\n",
       "      <td>0.547619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23272001</th>\n",
       "      <td>0.694949</td>\n",
       "      <td>0.259804</td>\n",
       "      <td>0.217213</td>\n",
       "      <td>0.323171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30261001</th>\n",
       "      <td>0.692593</td>\n",
       "      <td>0.260250</td>\n",
       "      <td>0.217262</td>\n",
       "      <td>0.324444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16893001</th>\n",
       "      <td>0.828148</td>\n",
       "      <td>0.573529</td>\n",
       "      <td>0.490566</td>\n",
       "      <td>0.690265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12900001</th>\n",
       "      <td>0.835354</td>\n",
       "      <td>0.543417</td>\n",
       "      <td>0.505208</td>\n",
       "      <td>0.587879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14023001</th>\n",
       "      <td>0.677778</td>\n",
       "      <td>0.325581</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2215001</th>\n",
       "      <td>0.602381</td>\n",
       "      <td>0.225657</td>\n",
       "      <td>0.167048</td>\n",
       "      <td>0.347619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21601001</th>\n",
       "      <td>0.662222</td>\n",
       "      <td>0.353191</td>\n",
       "      <td>0.259375</td>\n",
       "      <td>0.553333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2681001</th>\n",
       "      <td>0.846667</td>\n",
       "      <td>0.549020</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18531001</th>\n",
       "      <td>0.696970</td>\n",
       "      <td>0.257426</td>\n",
       "      <td>0.217573</td>\n",
       "      <td>0.315152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16637001</th>\n",
       "      <td>0.680808</td>\n",
       "      <td>0.373016</td>\n",
       "      <td>0.277286</td>\n",
       "      <td>0.569697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32459001</th>\n",
       "      <td>0.755556</td>\n",
       "      <td>0.382022</td>\n",
       "      <td>0.329032</td>\n",
       "      <td>0.455357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21120001</th>\n",
       "      <td>0.736667</td>\n",
       "      <td>0.439716</td>\n",
       "      <td>0.340659</td>\n",
       "      <td>0.620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1635001</th>\n",
       "      <td>0.753333</td>\n",
       "      <td>0.447761</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29063004</th>\n",
       "      <td>0.776768</td>\n",
       "      <td>0.413793</td>\n",
       "      <td>0.367925</td>\n",
       "      <td>0.472727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3942001</th>\n",
       "      <td>0.784848</td>\n",
       "      <td>0.521348</td>\n",
       "      <td>0.415771</td>\n",
       "      <td>0.698795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10729001</th>\n",
       "      <td>0.745185</td>\n",
       "      <td>0.330739</td>\n",
       "      <td>0.295139</td>\n",
       "      <td>0.376106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27946001</th>\n",
       "      <td>0.767460</td>\n",
       "      <td>0.372591</td>\n",
       "      <td>0.338521</td>\n",
       "      <td>0.414286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17436001</th>\n",
       "      <td>0.705556</td>\n",
       "      <td>0.379391</td>\n",
       "      <td>0.292419</td>\n",
       "      <td>0.540000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20688001</th>\n",
       "      <td>0.823016</td>\n",
       "      <td>0.443890</td>\n",
       "      <td>0.465969</td>\n",
       "      <td>0.423810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568001</th>\n",
       "      <td>0.678788</td>\n",
       "      <td>0.208955</td>\n",
       "      <td>0.177215</td>\n",
       "      <td>0.254545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23732001</th>\n",
       "      <td>0.630303</td>\n",
       "      <td>0.217949</td>\n",
       "      <td>0.167763</td>\n",
       "      <td>0.310976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25302001</th>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.269373</td>\n",
       "      <td>0.186224</td>\n",
       "      <td>0.486667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13252001</th>\n",
       "      <td>0.708889</td>\n",
       "      <td>0.288043</td>\n",
       "      <td>0.243119</td>\n",
       "      <td>0.353333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13640002</th>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.357955</td>\n",
       "      <td>0.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24540001</th>\n",
       "      <td>0.704444</td>\n",
       "      <td>0.314433</td>\n",
       "      <td>0.256303</td>\n",
       "      <td>0.406667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16266001</th>\n",
       "      <td>0.693333</td>\n",
       "      <td>0.269841</td>\n",
       "      <td>0.223684</td>\n",
       "      <td>0.340000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9809001</th>\n",
       "      <td>0.697619</td>\n",
       "      <td>0.320856</td>\n",
       "      <td>0.256410</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25922001</th>\n",
       "      <td>0.625556</td>\n",
       "      <td>0.293501</td>\n",
       "      <td>0.214067</td>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16779001</th>\n",
       "      <td>0.713131</td>\n",
       "      <td>0.330189</td>\n",
       "      <td>0.270270</td>\n",
       "      <td>0.424242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22337001</th>\n",
       "      <td>0.620202</td>\n",
       "      <td>0.298507</td>\n",
       "      <td>0.215054</td>\n",
       "      <td>0.487805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7385001</th>\n",
       "      <td>0.802222</td>\n",
       "      <td>0.552764</td>\n",
       "      <td>0.444744</td>\n",
       "      <td>0.730088</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Accuracy        F1  Precision    Recall\n",
       "subject                                          \n",
       "10924001  0.746825  0.405959   0.333333  0.519048\n",
       "22650001  0.589683  0.262482   0.187373  0.438095\n",
       "11693001  0.764646  0.371968   0.334951  0.418182\n",
       "26715001  0.677778  0.372294   0.275641  0.573333\n",
       "22109001  0.715556  0.396226   0.306569  0.560000\n",
       "31397001  0.753535  0.290698   0.279330  0.303030\n",
       "20668001  0.764444  0.442105   0.365217  0.560000\n",
       "29426001  0.663636  0.335329   0.250000  0.509091\n",
       "29257001  0.747778  0.349570   0.306533  0.406667\n",
       "10229001  0.683333  0.299754   0.237354  0.406667\n",
       "4824001   0.571111  0.215447   0.154971  0.353333\n",
       "23794001  0.688095  0.369181   0.278450  0.547619\n",
       "23272001  0.694949  0.259804   0.217213  0.323171\n",
       "30261001  0.692593  0.260250   0.217262  0.324444\n",
       "16893001  0.828148  0.573529   0.490566  0.690265\n",
       "12900001  0.835354  0.543417   0.505208  0.587879\n",
       "14023001  0.677778  0.325581   0.250000  0.466667\n",
       "2215001   0.602381  0.225657   0.167048  0.347619\n",
       "21601001  0.662222  0.353191   0.259375  0.553333\n",
       "2681001   0.846667  0.549020   0.538462  0.560000\n",
       "18531001  0.696970  0.257426   0.217573  0.315152\n",
       "16637001  0.680808  0.373016   0.277286  0.569697\n",
       "32459001  0.755556  0.382022   0.329032  0.455357\n",
       "21120001  0.736667  0.439716   0.340659  0.620000\n",
       "1635001   0.753333  0.447761   0.357143  0.600000\n",
       "29063004  0.776768  0.413793   0.367925  0.472727\n",
       "3942001   0.784848  0.521348   0.415771  0.698795\n",
       "10729001  0.745185  0.330739   0.295139  0.376106\n",
       "27946001  0.767460  0.372591   0.338521  0.414286\n",
       "17436001  0.705556  0.379391   0.292419  0.540000\n",
       "20688001  0.823016  0.443890   0.465969  0.423810\n",
       "5568001   0.678788  0.208955   0.177215  0.254545\n",
       "23732001  0.630303  0.217949   0.167763  0.310976\n",
       "25302001  0.560000  0.269373   0.186224  0.486667\n",
       "13252001  0.708889  0.288043   0.243119  0.353333\n",
       "13640002  0.760000  0.437500   0.357955  0.562500\n",
       "24540001  0.704444  0.314433   0.256303  0.406667\n",
       "16266001  0.693333  0.269841   0.223684  0.340000\n",
       "9809001   0.697619  0.320856   0.256410  0.428571\n",
       "25922001  0.625556  0.293501   0.214067  0.466667\n",
       "16779001  0.713131  0.330189   0.270270  0.424242\n",
       "22337001  0.620202  0.298507   0.215054  0.487805\n",
       "7385001   0.802222  0.552764   0.444744  0.730088"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(all_results)\n",
    "df.set_index(\"subject\", inplace=True)\n",
    "df.to_csv(\"results.csv\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Accuracy     0.712260\n",
       "F1           0.357327\n",
       "Precision    0.294537\n",
       "Recall       0.469452\n",
       "dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata\n",
    "\n",
    "Files we have trained our CNN with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training files: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['PruebasMuseo_27030001.set',\n",
       " 'PruebasMuseo_26636001.set',\n",
       " 'PruebasMuseo_21011001.set',\n",
       " 'PruebasMuseo_13235001.set',\n",
       " 'PruebasMuseo_16003001.set',\n",
       " 'PruebasMuseo_19561001.set',\n",
       " 'PruebasMuseo_24227001.set',\n",
       " 'PruebasMuseo_1491001.set',\n",
       " 'PruebasMuseo_29789001.set',\n",
       " 'PruebasMuseo_27131001.set',\n",
       " 'PruebasMuseo_6694001.set',\n",
       " 'PruebasMuseo_13863001.set',\n",
       " 'PruebasMuseo_24053001.set',\n",
       " 'PruebasMuseo_17005001.set',\n",
       " 'PruebasMuseo_14998001.set',\n",
       " 'PruebasMuseo_5857001.set',\n",
       " 'PruebasMuseo_22072001.set',\n",
       " 'PruebasMuseo_25871001.set',\n",
       " 'PruebasMuseo_5510001.set',\n",
       " 'PruebasMuseo_6830001.set',\n",
       " 'PruebasMuseo_26721001.set',\n",
       " 'PruebasMuseo_24101001.set',\n",
       " 'PruebasMuseo_27157001.set',\n",
       " 'PruebasMuseo_358001.set',\n",
       " 'PruebasMuseo_12702001.set',\n",
       " 'PruebasMuseo_9689001.set',\n",
       " 'PruebasMuseo_18967001.set',\n",
       " 'PruebasMuseo_16683001.set',\n",
       " 'PruebasMuseo_11627001.set',\n",
       " 'PruebasMuseo_31056001.set',\n",
       " 'PruebasMuseo_28005001.set',\n",
       " 'PruebasMuseo_2089001.set',\n",
       " 'PruebasMuseo_27846001.set',\n",
       " 'PruebasMuseo_12521001.set',\n",
       " 'PruebasMuseo_19845001.set',\n",
       " 'PruebasMuseo_18077001.set',\n",
       " 'PruebasMuseo_20947001.set',\n",
       " 'PruebasMuseo_9503001.set',\n",
       " 'PruebasMuseo_7615001.set',\n",
       " 'PruebasMuseo_27058001.set',\n",
       " 'PruebasMuseo_3800001.set',\n",
       " 'PruebasMuseo_24888001.set',\n",
       " 'PruebasMuseo_8762001.set',\n",
       " 'PruebasMuseo_8982001.set',\n",
       " 'PruebasMuseo_15362001.set',\n",
       " 'PruebasMuseo_4949001.set',\n",
       " 'PruebasMuseo_16943001.set',\n",
       " 'PruebasMuseo_10444001.set',\n",
       " 'PruebasMuseo_29164001.set',\n",
       " 'PruebasMuseo_36001.set',\n",
       " 'PruebasMuseo_2109001.set',\n",
       " 'PruebasMuseo_23344001.set',\n",
       " 'PruebasMuseo_5224001.set',\n",
       " 'PruebasMuseo_26333001.set',\n",
       " 'PruebasMuseo_499001.set',\n",
       " 'PruebasMuseo_630001.set',\n",
       " 'PruebasMuseo_8834001.set',\n",
       " 'PruebasMuseo_7330001.set',\n",
       " 'PruebasMuseo_21668001.set',\n",
       " 'PruebasMuseo_31777001.set',\n",
       " 'PruebasMuseo_7488001.set',\n",
       " 'PruebasMuseo_11632001.set',\n",
       " 'PruebasMuseo_12168001.set',\n",
       " 'PruebasMuseo_10882001.set',\n",
       " 'PruebasMuseo_31102001.set',\n",
       " 'PruebasMuseo_30243001.set',\n",
       " 'PruebasMuseo_4971001.set',\n",
       " 'PruebasMuseo_3703001.set',\n",
       " 'PruebasMuseo_17674001.set',\n",
       " 'PruebasMuseo_27163001.set',\n",
       " 'PruebasMuseo_22233001.set',\n",
       " 'PruebasMuseo_5251001.set',\n",
       " 'PruebasMuseo_17576001.set',\n",
       " 'PruebasMuseo_18112001.set',\n",
       " 'PruebasMuseo_19491001.set',\n",
       " 'PruebasMuseo_4305001.set',\n",
       " 'PruebasMuseo_15641001.set',\n",
       " 'PruebasMuseo_1609001.set',\n",
       " 'PruebasMuseo_25217001.set',\n",
       " 'PruebasMuseo_945001.set',\n",
       " 'PruebasMuseo_3195001.set',\n",
       " 'PruebasMuseo_15424001.set',\n",
       " 'PruebasMuseo_1414001.set',\n",
       " 'PruebasMuseo_28970001.set',\n",
       " 'PruebasMuseo_12137001.set',\n",
       " 'PruebasMuseo_782001.set',\n",
       " 'PruebasMuseo_23298001.set',\n",
       " 'PruebasMuseo_18306001.set',\n",
       " 'PruebasMuseo_17435001.set',\n",
       " 'PruebasMuseo_3109001.set',\n",
       " 'PruebasMuseo_29273001.set',\n",
       " 'PruebasMuseo_7610001.set',\n",
       " 'PruebasMuseo_32505001.set',\n",
       " 'PruebasMuseo_232001.set',\n",
       " 'PruebasMuseo_255001.set',\n",
       " 'PruebasMuseo_11551001.set',\n",
       " 'PruebasMuseo_18046001.set',\n",
       " 'PruebasMuseo_13431001.set',\n",
       " 'PruebasMuseo_27496001.set',\n",
       " 'PruebasMuseo_17962001.set']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Training files: \")\n",
    "[path.split(\"/\")[-1] for path in training_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
