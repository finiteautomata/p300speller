{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN for many subjects\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero:\n",
    "\n",
    "Cargamos los datos y los normalizamos. Para esto, primero pasamos un filtro pasa-bajo de 0 a 20hz, luego lo normalizamos a $N(0, 1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: TkAgg\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jmperez/.pyenv/versions/3.6.5/envs/p300/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU's disponibles = ['/job:localhost/replica:0/task:0/device:GPU:0']\n"
     ]
    }
   ],
   "source": [
    "%pylab\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "if \"..\" not in sys.path:\n",
    "    sys.path.append(\"..\")\n",
    "import glob\n",
    "import os\n",
    "import mne\n",
    "from keras import backend as K\n",
    "from p300.preprocessing import normalize_subject, load_data\n",
    "\n",
    "print(\"GPU's disponibles = {}\".format(K.tensorflow_backend._get_available_gpus()))\n",
    "\n",
    "CORPORA_PATH = \"~/projects/corpora/P3Speller/P3Speller-old-y-datos/sets\"\n",
    "\n",
    "file_path = os.path.expanduser(CORPORA_PATH)\n",
    "files = sorted(glob.glob(os.path.join(file_path, \"*.set\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Targets appear as 2 in the third column\n",
    "\n",
    "\n",
    "We remove last channel as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%%time \n",
    "\n",
    "# this line is to avoid output\n",
    "pretraining_no = 100\n",
    "\n",
    "training_files = files[:pretraining_no]\n",
    "testing_files = files[pretraining_no:]\n",
    "\n",
    "X_train, y_train = load_data(training_files)\n",
    "# Check that there are no overlaps!\n",
    "assert(len([f for f in training_files if f in testing_files]) == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {0: 0.6, 1: 3.0}\n",
      "(196380, 14, 104, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "y_t = y_train.reshape(-1)\n",
    "class_weights = class_weight.compute_class_weight('balanced', np.unique(y_t), y_t)\n",
    "\n",
    "class_weights = dict(zip([0,1], class_weights))\n",
    "\n",
    "print(\"Class weights: {}\".format(class_weights))\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jmperez/.pyenv/versions/3.6.5/envs/p300/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1264: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/jmperez/.pyenv/versions/3.6.5/envs/p300/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1349: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, Conv2D, Flatten, Dense, Dropout\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    activation = 'relu'\n",
    "\n",
    "    n_kernels = 12\n",
    "    model.add(Conv2D(n_kernels, (14, 1), padding='same',\n",
    "                    activation=activation, input_shape=(14, 104, 1)))\n",
    "    model.add(Conv2D(5*n_kernels, (1, 13), padding='same',\n",
    "                    activation=activation))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.45))\n",
    "    model.add(Dense(128, activation=activation))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', # using the cross-entropy loss function\n",
    "                  optimizer='rmsprop', \n",
    "                  metrics=['accuracy']) # reporting the accuracy\n",
    "    return model\n",
    "\n",
    "\n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 176742 samples, validate on 19638 samples\n",
      "Epoch 1/40\n",
      "176384/176742 [============================>.] - ETA: 0s - loss: 0.6921 - acc: 0.5987Epoch 00001: val_loss improved from inf to 0.67057, saving model to models/model_cnn_1.h5\n",
      "176742/176742 [==============================] - 24s 137us/step - loss: 0.6920 - acc: 0.5988 - val_loss: 0.6706 - val_acc: 0.6970\n",
      "Epoch 2/40\n",
      "176640/176742 [============================>.] - ETA: 0s - loss: 0.6639 - acc: 0.6348Epoch 00002: val_loss improved from 0.67057 to 0.66438, saving model to models/model_cnn_1.h5\n",
      "176742/176742 [==============================] - 18s 104us/step - loss: 0.6640 - acc: 0.6348 - val_loss: 0.6644 - val_acc: 0.5626\n",
      "Epoch 3/40\n",
      "176640/176742 [============================>.] - ETA: 0s - loss: 0.6591 - acc: 0.6389Epoch 00003: val_loss did not improve\n",
      "176742/176742 [==============================] - 18s 104us/step - loss: 0.6590 - acc: 0.6389 - val_loss: 0.6749 - val_acc: 0.7646\n",
      "Epoch 4/40\n",
      "176128/176742 [============================>.] - ETA: 0s - loss: 0.6554 - acc: 0.6478Epoch 00004: val_loss improved from 0.66438 to 0.66202, saving model to models/model_cnn_1.h5\n",
      "176742/176742 [==============================] - 19s 105us/step - loss: 0.6555 - acc: 0.6478 - val_loss: 0.6620 - val_acc: 0.5958\n",
      "Epoch 5/40\n",
      "176640/176742 [============================>.] - ETA: 0s - loss: 0.6527 - acc: 0.6633Epoch 00005: val_loss did not improve\n",
      "176742/176742 [==============================] - 18s 104us/step - loss: 0.6527 - acc: 0.6633 - val_loss: 0.6642 - val_acc: 0.6286\n",
      "Epoch 6/40\n",
      "176640/176742 [============================>.] - ETA: 0s - loss: 0.6515 - acc: 0.6753Epoch 00006: val_loss improved from 0.66202 to 0.66165, saving model to models/model_cnn_1.h5\n",
      "176742/176742 [==============================] - 19s 105us/step - loss: 0.6515 - acc: 0.6752 - val_loss: 0.6617 - val_acc: 0.6836\n",
      "Epoch 7/40\n",
      "176384/176742 [============================>.] - ETA: 0s - loss: 0.6507 - acc: 0.6737Epoch 00007: val_loss did not improve\n",
      "176742/176742 [==============================] - 18s 104us/step - loss: 0.6507 - acc: 0.6737 - val_loss: 0.6672 - val_acc: 0.7052\n",
      "Epoch 8/40\n",
      "176384/176742 [============================>.] - ETA: 0s - loss: 0.6517 - acc: 0.6773Epoch 00008: val_loss did not improve\n",
      "176742/176742 [==============================] - 18s 104us/step - loss: 0.6517 - acc: 0.6773 - val_loss: 0.6826 - val_acc: 0.7070\n",
      "Epoch 9/40\n",
      "176384/176742 [============================>.] - ETA: 0s - loss: 0.6521 - acc: 0.6811Epoch 00009: val_loss did not improve\n",
      "176742/176742 [==============================] - 18s 104us/step - loss: 0.6522 - acc: 0.6811 - val_loss: 0.6649 - val_acc: 0.7195\n",
      "Epoch 10/40\n",
      "176128/176742 [============================>.] - ETA: 0s - loss: 0.6514 - acc: 0.6778Epoch 00010: val_loss did not improve\n",
      "176742/176742 [==============================] - 18s 104us/step - loss: 0.6513 - acc: 0.6778 - val_loss: 0.6643 - val_acc: 0.6649\n",
      "Epoch 11/40\n",
      "176384/176742 [============================>.] - ETA: 0s - loss: 0.6513 - acc: 0.6767Epoch 00011: val_loss did not improve\n",
      "176742/176742 [==============================] - 18s 104us/step - loss: 0.6514 - acc: 0.6766 - val_loss: 0.6641 - val_acc: 0.6829\n",
      "CPU times: user 2min 56s, sys: 41.3 s, total: 3min 37s\n",
      "Wall time: 3min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "checkpointer = ModelCheckpoint(filepath='models/model_cnn_1.h5', verbose=1, save_best_only=True)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train, epochs=40, \n",
    "    batch_size=256, class_weight=class_weights, validation_split=0.10,\n",
    "    callbacks=[checkpointer, early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretraining\n",
    "\n",
    "Let's fix the first four layers, and retrain the dense layers only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<keras.layers.convolutional.Conv2D at 0x7f71025dc0b8>, 'Trainable: False'),\n",
       " (<keras.layers.convolutional.Conv2D at 0x7f71025dc400>, 'Trainable: False'),\n",
       " (<keras.layers.core.Flatten at 0x7f71025dcac8>, 'Trainable: False'),\n",
       " (<keras.layers.core.Dropout at 0x7f710512eb00>, 'Trainable: False'),\n",
       " (<keras.layers.core.Dense at 0x7f710512ea58>, 'Trainable: True'),\n",
       " (<keras.layers.core.Dense at 0x7f710506bcf8>, 'Trainable: True')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def fix_layers(model, fixed_layers):\n",
    "    for i in range(fixed_layers):\n",
    "        model.layers[i].trainable = False\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop', \n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "fix_layers(model, 4)    \n",
    "\n",
    "[(l, \"Trainable: {}\".format(l.trainable)) for l in model.layers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the idea is to train each subject and fine tune the last layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture output\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import (\n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    roc_auc_score, \n",
    "    accuracy_score, \n",
    "    f1_score\n",
    ")\n",
    "from p300.preprocessing import normalize_subject, load_data, load_data_from_subject\n",
    "\n",
    "\n",
    "def get_fine_tune_results(model_path, file):\n",
    "    K.clear_session()\n",
    "    \n",
    "    model = load_model(model_path)\n",
    "    \n",
    "    fix_layers(model, 4)\n",
    "\n",
    "    X_sub, y_sub = load_data([file])\n",
    "    \n",
    "    if X_sub is None:\n",
    "        return \n",
    "\n",
    "    length = X_sub.shape[0] \n",
    "    limit = int(length / 2)\n",
    "    X_sub_train, X_sub_test = X_sub[:limit], X_sub[limit:]\n",
    "    y_sub_train, y_sub_test = y_sub[:limit], y_sub[limit:]\n",
    "    \n",
    "    model.fit(\n",
    "        X_sub_train, y_sub_train, epochs=20, \n",
    "        batch_size=64, class_weight=class_weights, validation_split=0.1,\n",
    "    )\n",
    "    \n",
    "    y_pred = model.predict_classes(X_sub_test)\n",
    "    y_prob = model.predict(X_sub_test)\n",
    "\n",
    "    precision = precision_score(y_sub_test, y_pred)\n",
    "    recall = recall_score(y_sub_test, y_pred)\n",
    "    auc = roc_auc_score(y_sub_test, y_prob)\n",
    "    accuracy = accuracy_score(y_sub_test, y_pred)\n",
    "    f1 = f1_score(y_sub_test, y_pred)\n",
    "    \n",
    "    subject_name = file.split(\"/\")[-1].split(\".\")[0].split(\"_\")[-1]\n",
    "    K.clear_session()\n",
    "    return {\n",
    "        \"subject\": subject_name,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1\": f1\n",
    "    }, model\n",
    "    \n",
    "\n",
    "\n",
    "model_path = 'models/model_cnn_1.h5'\n",
    "\n",
    "all_results = []\n",
    "for file in testing_files:\n",
    "    results = get_fine_tune_results(model_path, file)\n",
    "    if results is None:\n",
    "        print(\"Skipping {}\")\n",
    "        continue\n",
    "    all_results.append(results[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29257001</th>\n",
       "      <td>0.761111</td>\n",
       "      <td>0.304207</td>\n",
       "      <td>0.295597</td>\n",
       "      <td>0.313333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29273001</th>\n",
       "      <td>0.828889</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.489247</td>\n",
       "      <td>0.606667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29426001</th>\n",
       "      <td>0.734343</td>\n",
       "      <td>0.340852</td>\n",
       "      <td>0.290598</td>\n",
       "      <td>0.412121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29789001</th>\n",
       "      <td>0.822222</td>\n",
       "      <td>0.480519</td>\n",
       "      <td>0.468354</td>\n",
       "      <td>0.493333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30243001</th>\n",
       "      <td>0.798990</td>\n",
       "      <td>0.387692</td>\n",
       "      <td>0.393750</td>\n",
       "      <td>0.381818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30261001</th>\n",
       "      <td>0.691111</td>\n",
       "      <td>0.199616</td>\n",
       "      <td>0.175676</td>\n",
       "      <td>0.231111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31056001</th>\n",
       "      <td>0.708148</td>\n",
       "      <td>0.245211</td>\n",
       "      <td>0.215488</td>\n",
       "      <td>0.284444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3109001</th>\n",
       "      <td>0.765657</td>\n",
       "      <td>0.411168</td>\n",
       "      <td>0.353712</td>\n",
       "      <td>0.490909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31102001</th>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.273256</td>\n",
       "      <td>0.242268</td>\n",
       "      <td>0.313333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31397001</th>\n",
       "      <td>0.739394</td>\n",
       "      <td>0.279330</td>\n",
       "      <td>0.259067</td>\n",
       "      <td>0.303030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31777001</th>\n",
       "      <td>0.677778</td>\n",
       "      <td>0.197628</td>\n",
       "      <td>0.168919</td>\n",
       "      <td>0.238095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3195001</th>\n",
       "      <td>0.777037</td>\n",
       "      <td>0.360934</td>\n",
       "      <td>0.344130</td>\n",
       "      <td>0.379464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32459001</th>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.367188</td>\n",
       "      <td>0.326389</td>\n",
       "      <td>0.419643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32505001</th>\n",
       "      <td>0.721212</td>\n",
       "      <td>0.306533</td>\n",
       "      <td>0.262931</td>\n",
       "      <td>0.367470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358001</th>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.282209</td>\n",
       "      <td>0.261364</td>\n",
       "      <td>0.306667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36001</th>\n",
       "      <td>0.718889</td>\n",
       "      <td>0.181230</td>\n",
       "      <td>0.176101</td>\n",
       "      <td>0.186667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3703001</th>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.185345</td>\n",
       "      <td>0.169291</td>\n",
       "      <td>0.204762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3800001</th>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.372093</td>\n",
       "      <td>0.329897</td>\n",
       "      <td>0.426667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3942001</th>\n",
       "      <td>0.815152</td>\n",
       "      <td>0.475645</td>\n",
       "      <td>0.453552</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4305001</th>\n",
       "      <td>0.794949</td>\n",
       "      <td>0.297578</td>\n",
       "      <td>0.346774</td>\n",
       "      <td>0.260606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4824001</th>\n",
       "      <td>0.623333</td>\n",
       "      <td>0.234763</td>\n",
       "      <td>0.177474</td>\n",
       "      <td>0.346667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4949001</th>\n",
       "      <td>0.731852</td>\n",
       "      <td>0.252066</td>\n",
       "      <td>0.234615</td>\n",
       "      <td>0.272321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4971001</th>\n",
       "      <td>0.657576</td>\n",
       "      <td>0.331361</td>\n",
       "      <td>0.244898</td>\n",
       "      <td>0.512195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499001</th>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.277108</td>\n",
       "      <td>0.252747</td>\n",
       "      <td>0.306667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5224001</th>\n",
       "      <td>0.768687</td>\n",
       "      <td>0.254072</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.237805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5251001</th>\n",
       "      <td>0.823333</td>\n",
       "      <td>0.376471</td>\n",
       "      <td>0.457143</td>\n",
       "      <td>0.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5510001</th>\n",
       "      <td>0.747475</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.240964</td>\n",
       "      <td>0.243902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568001</th>\n",
       "      <td>0.723232</td>\n",
       "      <td>0.184524</td>\n",
       "      <td>0.181287</td>\n",
       "      <td>0.187879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5857001</th>\n",
       "      <td>0.881818</td>\n",
       "      <td>0.597938</td>\n",
       "      <td>0.696000</td>\n",
       "      <td>0.524096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630001</th>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.345455</td>\n",
       "      <td>0.316667</td>\n",
       "      <td>0.380000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6694001</th>\n",
       "      <td>0.755556</td>\n",
       "      <td>0.349462</td>\n",
       "      <td>0.314010</td>\n",
       "      <td>0.393939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6830001</th>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.237288</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.254545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7330001</th>\n",
       "      <td>0.734444</td>\n",
       "      <td>0.255452</td>\n",
       "      <td>0.239766</td>\n",
       "      <td>0.273333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7385001</th>\n",
       "      <td>0.831852</td>\n",
       "      <td>0.541414</td>\n",
       "      <td>0.498141</td>\n",
       "      <td>0.592920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7488001</th>\n",
       "      <td>0.714444</td>\n",
       "      <td>0.328982</td>\n",
       "      <td>0.270386</td>\n",
       "      <td>0.420000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610001</th>\n",
       "      <td>0.768687</td>\n",
       "      <td>0.282132</td>\n",
       "      <td>0.292208</td>\n",
       "      <td>0.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7615001</th>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.273973</td>\n",
       "      <td>0.363636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782001</th>\n",
       "      <td>0.762698</td>\n",
       "      <td>0.373166</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.423810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8762001</th>\n",
       "      <td>0.731746</td>\n",
       "      <td>0.292887</td>\n",
       "      <td>0.261194</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8834001</th>\n",
       "      <td>0.675556</td>\n",
       "      <td>0.391667</td>\n",
       "      <td>0.284848</td>\n",
       "      <td>0.626667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8982001</th>\n",
       "      <td>0.747778</td>\n",
       "      <td>0.255738</td>\n",
       "      <td>0.251613</td>\n",
       "      <td>0.260000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>945001</th>\n",
       "      <td>0.693333</td>\n",
       "      <td>0.321311</td>\n",
       "      <td>0.255208</td>\n",
       "      <td>0.433628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9503001</th>\n",
       "      <td>0.753535</td>\n",
       "      <td>0.364583</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.426829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9689001</th>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.219388</td>\n",
       "      <td>0.177686</td>\n",
       "      <td>0.286667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9809001</th>\n",
       "      <td>0.717460</td>\n",
       "      <td>0.288000</td>\n",
       "      <td>0.248276</td>\n",
       "      <td>0.342857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Accuracy        F1  Precision    Recall\n",
       "subject                                          \n",
       "29257001  0.761111  0.304207   0.295597  0.313333\n",
       "29273001  0.828889  0.541667   0.489247  0.606667\n",
       "29426001  0.734343  0.340852   0.290598  0.412121\n",
       "29789001  0.822222  0.480519   0.468354  0.493333\n",
       "30243001  0.798990  0.387692   0.393750  0.381818\n",
       "30261001  0.691111  0.199616   0.175676  0.231111\n",
       "31056001  0.708148  0.245211   0.215488  0.284444\n",
       "3109001   0.765657  0.411168   0.353712  0.490909\n",
       "31102001  0.722222  0.273256   0.242268  0.313333\n",
       "31397001  0.739394  0.279330   0.259067  0.303030\n",
       "31777001  0.677778  0.197628   0.168919  0.238095\n",
       "3195001   0.777037  0.360934   0.344130  0.379464\n",
       "32459001  0.760000  0.367188   0.326389  0.419643\n",
       "32505001  0.721212  0.306533   0.262931  0.367470\n",
       "358001    0.740000  0.282209   0.261364  0.306667\n",
       "36001     0.718889  0.181230   0.176101  0.186667\n",
       "3703001   0.700000  0.185345   0.169291  0.204762\n",
       "3800001   0.760000  0.372093   0.329897  0.426667\n",
       "3942001   0.815152  0.475645   0.453552  0.500000\n",
       "4305001   0.794949  0.297578   0.346774  0.260606\n",
       "4824001   0.623333  0.234763   0.177474  0.346667\n",
       "4949001   0.731852  0.252066   0.234615  0.272321\n",
       "4971001   0.657576  0.331361   0.244898  0.512195\n",
       "499001    0.733333  0.277108   0.252747  0.306667\n",
       "5224001   0.768687  0.254072   0.272727  0.237805\n",
       "5251001   0.823333  0.376471   0.457143  0.320000\n",
       "5510001   0.747475  0.242424   0.240964  0.243902\n",
       "5568001   0.723232  0.184524   0.181287  0.187879\n",
       "5857001   0.881818  0.597938   0.696000  0.524096\n",
       "630001    0.760000  0.345455   0.316667  0.380000\n",
       "6694001   0.755556  0.349462   0.314010  0.393939\n",
       "6830001   0.727273  0.237288   0.222222  0.254545\n",
       "7330001   0.734444  0.255452   0.239766  0.273333\n",
       "7385001   0.831852  0.541414   0.498141  0.592920\n",
       "7488001   0.714444  0.328982   0.270386  0.420000\n",
       "7610001   0.768687  0.282132   0.292208  0.272727\n",
       "7615001   0.733333  0.312500   0.273973  0.363636\n",
       "782001    0.762698  0.373166   0.333333  0.423810\n",
       "8762001   0.731746  0.292887   0.261194  0.333333\n",
       "8834001   0.675556  0.391667   0.284848  0.626667\n",
       "8982001   0.747778  0.255738   0.251613  0.260000\n",
       "945001    0.693333  0.321311   0.255208  0.433628\n",
       "9503001   0.753535  0.364583   0.318182  0.426829\n",
       "9689001   0.660000  0.219388   0.177686  0.286667\n",
       "9809001   0.717460  0.288000   0.248276  0.342857"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(all_results)\n",
    "df.set_index(\"subject\", inplace=True)\n",
    "df.to_csv(\"results.csv\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Accuracy     0.744343\n",
       "F1           0.320001\n",
       "Precision    0.296415\n",
       "Recall       0.359035\n",
       "dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy     0.779232\n",
    "F1           0.406924\n",
    "Precision    0.373494\n",
    "Recall       0.457576\n",
    "dtype: float64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture output\n",
    "\n",
    "\n",
    "def get_fine_tune_results(model_path, file):\n",
    "    K.clear_session()\n",
    "    \n",
    "    model = create_model()\n",
    "\n",
    "    X_sub, y_sub = load_data([file])\n",
    "    \n",
    "    if X_sub is None:\n",
    "        return\n",
    "\n",
    "    length = X_sub.shape[0] \n",
    "    limit = int(length / 2)\n",
    "    X_sub_train, X_sub_test = X_sub[:limit], X_sub[limit:]\n",
    "    y_sub_train, y_sub_test = y_sub[:limit], y_sub[limit:]\n",
    "    \n",
    "    model.fit(\n",
    "        X_sub_train, y_sub_train, epochs=20, \n",
    "        batch_size=64, class_weight=class_weights, validation_split=0.01,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    \n",
    "    y_pred = model.predict_classes(X_sub_test)\n",
    "    y_prob = model.predict(X_sub_test)\n",
    "\n",
    "    precision = precision_score(y_sub_test, y_pred)\n",
    "    recall = recall_score(y_sub_test, y_pred)\n",
    "    auc = roc_auc_score(y_sub_test, y_prob)\n",
    "    accuracy = accuracy_score(y_sub_test, y_pred)\n",
    "    f1 = f1_score(y_sub_test, y_pred)\n",
    "    \n",
    "    subject_name = file.split(\"/\")[-1].split(\".\")[0].split(\"_\")[-1]\n",
    "    K.clear_session()\n",
    "    return {\n",
    "        \"subject\": subject_name,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1\": f1\n",
    "    }, model\n",
    "    \n",
    "\n",
    "\n",
    "model_path = 'models/model_cnn_1.h5'\n",
    "\n",
    "nft_all_results = []\n",
    "for file in testing_files:\n",
    "    results = get_fine_tune_results(model_path, file)\n",
    "    if results is None:\n",
    "        print(\"Skipping {}\")\n",
    "        continue\n",
    "    nft_all_results.append(results[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Accuracy     0.744764\n",
       "F1           0.172782\n",
       "Precision    0.266223\n",
       "Recall       0.211941\n",
       "dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_without = pd.DataFrame(nft_all_results)\n",
    "df_without.set_index(\"subject\", inplace=True)\n",
    "df_without.to_csv(\"results_without.csv\")\n",
    "\n",
    "df_without.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata\n",
    "\n",
    "Files we have trained our CNN with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training files: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['PruebasMuseo_10229001.set',\n",
       " 'PruebasMuseo_10444001.set',\n",
       " 'PruebasMuseo_10729001.set',\n",
       " 'PruebasMuseo_10882001.set',\n",
       " 'PruebasMuseo_10924001.set',\n",
       " 'PruebasMuseo_11551001.set',\n",
       " 'PruebasMuseo_11627001.set',\n",
       " 'PruebasMuseo_11632001.set',\n",
       " 'PruebasMuseo_11693001.set',\n",
       " 'PruebasMuseo_12137001.set',\n",
       " 'PruebasMuseo_12168001.set',\n",
       " 'PruebasMuseo_12521001.set',\n",
       " 'PruebasMuseo_12702001.set',\n",
       " 'PruebasMuseo_12900001.set',\n",
       " 'PruebasMuseo_13235001.set',\n",
       " 'PruebasMuseo_13252001.set',\n",
       " 'PruebasMuseo_13431001.set',\n",
       " 'PruebasMuseo_13640002.set',\n",
       " 'PruebasMuseo_13863001.set',\n",
       " 'PruebasMuseo_14023001.set',\n",
       " 'PruebasMuseo_1414001.set',\n",
       " 'PruebasMuseo_1491001.set',\n",
       " 'PruebasMuseo_14998001.set',\n",
       " 'PruebasMuseo_15362001.set',\n",
       " 'PruebasMuseo_15424001.set',\n",
       " 'PruebasMuseo_15641001.set',\n",
       " 'PruebasMuseo_16003001.set',\n",
       " 'PruebasMuseo_1609001.set',\n",
       " 'PruebasMuseo_16266001.set',\n",
       " 'PruebasMuseo_1635001.set',\n",
       " 'PruebasMuseo_16637001.set',\n",
       " 'PruebasMuseo_16683001.set',\n",
       " 'PruebasMuseo_16779001.set',\n",
       " 'PruebasMuseo_16893001.set',\n",
       " 'PruebasMuseo_16943001.set',\n",
       " 'PruebasMuseo_17005001.set',\n",
       " 'PruebasMuseo_17435001.set',\n",
       " 'PruebasMuseo_17436001.set',\n",
       " 'PruebasMuseo_17576001.set',\n",
       " 'PruebasMuseo_17674001.set',\n",
       " 'PruebasMuseo_17962001.set',\n",
       " 'PruebasMuseo_18046001.set',\n",
       " 'PruebasMuseo_18077001.set',\n",
       " 'PruebasMuseo_18112001.set',\n",
       " 'PruebasMuseo_18306001.set',\n",
       " 'PruebasMuseo_18531001.set',\n",
       " 'PruebasMuseo_18967001.set',\n",
       " 'PruebasMuseo_19491001.set',\n",
       " 'PruebasMuseo_19561001.set',\n",
       " 'PruebasMuseo_19845001.set',\n",
       " 'PruebasMuseo_20668001.set',\n",
       " 'PruebasMuseo_20688001.set',\n",
       " 'PruebasMuseo_2089001.set',\n",
       " 'PruebasMuseo_20947001.set',\n",
       " 'PruebasMuseo_21011001.set',\n",
       " 'PruebasMuseo_2109001.set',\n",
       " 'PruebasMuseo_21120001.set',\n",
       " 'PruebasMuseo_21601001.set',\n",
       " 'PruebasMuseo_21668001.set',\n",
       " 'PruebasMuseo_22072001.set',\n",
       " 'PruebasMuseo_22109001.set',\n",
       " 'PruebasMuseo_2215001.set',\n",
       " 'PruebasMuseo_22233001.set',\n",
       " 'PruebasMuseo_22337001.set',\n",
       " 'PruebasMuseo_22650001.set',\n",
       " 'PruebasMuseo_232001.set',\n",
       " 'PruebasMuseo_23272001.set',\n",
       " 'PruebasMuseo_23298001.set',\n",
       " 'PruebasMuseo_23344001.set',\n",
       " 'PruebasMuseo_23732001.set',\n",
       " 'PruebasMuseo_23794001.set',\n",
       " 'PruebasMuseo_24053001.set',\n",
       " 'PruebasMuseo_24101001.set',\n",
       " 'PruebasMuseo_24227001.set',\n",
       " 'PruebasMuseo_24540001.set',\n",
       " 'PruebasMuseo_24888001.set',\n",
       " 'PruebasMuseo_25217001.set',\n",
       " 'PruebasMuseo_25302001.set',\n",
       " 'PruebasMuseo_255001.set',\n",
       " 'PruebasMuseo_25871001.set',\n",
       " 'PruebasMuseo_25922001.set',\n",
       " 'PruebasMuseo_26333001.set',\n",
       " 'PruebasMuseo_26636001.set',\n",
       " 'PruebasMuseo_26715001.set',\n",
       " 'PruebasMuseo_26721001.set',\n",
       " 'PruebasMuseo_2681001.set',\n",
       " 'PruebasMuseo_27030001.set',\n",
       " 'PruebasMuseo_27058001.set',\n",
       " 'PruebasMuseo_27131001.set',\n",
       " 'PruebasMuseo_27157001.set',\n",
       " 'PruebasMuseo_27163001.set',\n",
       " 'PruebasMuseo_27408001.set',\n",
       " 'PruebasMuseo_27496001.set',\n",
       " 'PruebasMuseo_27846001.set',\n",
       " 'PruebasMuseo_27946001.set',\n",
       " 'PruebasMuseo_28005001.set',\n",
       " 'PruebasMuseo_28301001.set',\n",
       " 'PruebasMuseo_28970001.set',\n",
       " 'PruebasMuseo_29063004.set',\n",
       " 'PruebasMuseo_29164001.set']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Training files: \")\n",
    "[path.split(\"/\")[-1] for path in training_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing files: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['PruebasMuseo_29257001.set',\n",
       " 'PruebasMuseo_29273001.set',\n",
       " 'PruebasMuseo_29426001.set',\n",
       " 'PruebasMuseo_29789001.set',\n",
       " 'PruebasMuseo_30243001.set',\n",
       " 'PruebasMuseo_30261001.set',\n",
       " 'PruebasMuseo_31056001.set',\n",
       " 'PruebasMuseo_3109001.set',\n",
       " 'PruebasMuseo_31102001.set',\n",
       " 'PruebasMuseo_31397001.set',\n",
       " 'PruebasMuseo_31777001.set',\n",
       " 'PruebasMuseo_3195001.set',\n",
       " 'PruebasMuseo_32459001.set',\n",
       " 'PruebasMuseo_32505001.set',\n",
       " 'PruebasMuseo_358001.set',\n",
       " 'PruebasMuseo_36001.set',\n",
       " 'PruebasMuseo_3703001.set',\n",
       " 'PruebasMuseo_3800001.set',\n",
       " 'PruebasMuseo_3942001.set',\n",
       " 'PruebasMuseo_4305001.set',\n",
       " 'PruebasMuseo_4824001.set',\n",
       " 'PruebasMuseo_4949001.set',\n",
       " 'PruebasMuseo_4971001.set',\n",
       " 'PruebasMuseo_499001.set',\n",
       " 'PruebasMuseo_5224001.set',\n",
       " 'PruebasMuseo_5251001.set',\n",
       " 'PruebasMuseo_5510001.set',\n",
       " 'PruebasMuseo_5568001.set',\n",
       " 'PruebasMuseo_5857001.set',\n",
       " 'PruebasMuseo_630001.set',\n",
       " 'PruebasMuseo_6694001.set',\n",
       " 'PruebasMuseo_6830001.set',\n",
       " 'PruebasMuseo_7330001.set',\n",
       " 'PruebasMuseo_7385001.set',\n",
       " 'PruebasMuseo_7488001.set',\n",
       " 'PruebasMuseo_7610001.set',\n",
       " 'PruebasMuseo_7615001.set',\n",
       " 'PruebasMuseo_782001.set',\n",
       " 'PruebasMuseo_8762001.set',\n",
       " 'PruebasMuseo_8834001.set',\n",
       " 'PruebasMuseo_8982001.set',\n",
       " 'PruebasMuseo_945001.set',\n",
       " 'PruebasMuseo_9503001.set',\n",
       " 'PruebasMuseo_9689001.set',\n",
       " 'PruebasMuseo_9809001.set']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Testing files: \")\n",
    "[path.split(\"/\")[-1] for path in testing_files]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
