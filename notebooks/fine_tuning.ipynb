{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning of the convolutional network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: TkAgg\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab\n",
    "%matplotlib inline\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import mne\n",
    "CORPORA_PATH = \"~/corpora/sets\"\n",
    "\n",
    "file_path = os.path.expanduser(CORPORA_PATH)\n",
    "files = glob.glob(os.path.join(file_path, \"*.set\"))\n",
    "\n",
    "def normalize_subject(X):\n",
    "    mean = X.mean(axis=(0, 2)).reshape(-1, 1)\n",
    "    std = X.std(axis=(0, 2)).reshape(-1, 1)\n",
    "    return (X - mean) / std\n",
    "\n",
    "def load_data(filename, normalize=True):\n",
    "    data_mne = mne.io.read_raw_eeglab(filename, preload=True, event_id={\"0\": 1, \"1\": 2})\n",
    "    data_mne.filter(0, 20)\n",
    "    events = mne.find_events(data_mne)\n",
    "    epochs = mne.Epochs(\n",
    "        data_mne, events,\n",
    "        baseline=(None, 0), tmin=-0.1, tmax=0.7)\n",
    "\n",
    "    epochs.load_data()\n",
    "    \n",
    "    ch_names = epochs.ch_names\n",
    "    \n",
    "    X = epochs.get_data()[:, :-1]\n",
    "    y = (events[:, 2] == 2).astype('float')\n",
    "\n",
    "    if len(events) != len(epochs):\n",
    "        raise ValueError(\"Epochs events mismatch\")\n",
    "    if normalize: \n",
    "        X = normalize_subject(X)\n",
    "    X = X[..., np.newaxis]\n",
    "    \n",
    "    return X, y \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/jmperez/.pyenv/versions/3.6.0/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "channels = ['AF3', 'F7', 'F3', 'FC5', 'T7', 'P7', 'O1', 'O2', 'P8', 'T8', 'FC6', 'F4', 'F8', 'AF4', 'STI 014']\n",
    "\n",
    "model = load_model(\"models/model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /home/jmperez/corpora/sets/PruebasMuseo_358001.fdt\n",
      "Reading 0 ... 63231  =      0.000 ...   493.992 secs...\n",
      "Setting up low-pass filter at 20 Hz\n",
      "h_trans_bandwidth chosen to be 5.0 Hz\n",
      "Filter length of 169 samples (1.320 sec) selected\n",
      "1800 events found\n",
      "Events id: [1 2]\n",
      "1800 matching events found\n",
      "0 projection items activated\n",
      "Loading data for 1800 events and 104 original time points ...\n",
      "0 bad epochs dropped\n"
     ]
    }
   ],
   "source": [
    "X, y = load_data(files[143])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.1, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.56111111111111112,\n",
       " 'precision': 0.21176470588235294,\n",
       " 'recall': 0.59999999999999998,\n",
       " 'roc_auc': 0.57822222222222219}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score, accuracy_score\n",
    "\n",
    "def get_metrics(model, X_test, y_test):\n",
    "    y_pred = model.predict_classes(X_test)\n",
    "    y_prob = model.predict(X_test)\n",
    "\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_prob)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall, \n",
    "        \"roc_auc\": auc\n",
    "    }\n",
    "    \n",
    "    \n",
    "get_metrics(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.convolutional.Conv2D at 0x7efe0a358c18>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7efe0a358f28>,\n",
       " <keras.layers.core.Flatten at 0x7efe0a358ef0>,\n",
       " <keras.layers.core.Dropout at 0x7efe0a3204a8>,\n",
       " <keras.layers.core.Dense at 0x7efe0a317dd8>,\n",
       " <keras.layers.core.Dense at 0x7efe0a2c7fd0>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fix the first two convolutional layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<keras.layers.convolutional.Conv2D at 0x7efe0a358c18>, 'Trainable: False'),\n",
       " (<keras.layers.convolutional.Conv2D at 0x7efe0a358f28>, 'Trainable: False'),\n",
       " (<keras.layers.core.Flatten at 0x7efe0a358ef0>, 'Trainable: False'),\n",
       " (<keras.layers.core.Dropout at 0x7efe0a3204a8>, 'Trainable: False'),\n",
       " (<keras.layers.core.Dense at 0x7efe0a317dd8>, 'Trainable: True'),\n",
       " (<keras.layers.core.Dense at 0x7efe0a2c7fd0>, 'Trainable: True')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "for i in range(4):\n",
    "    model.layers[i].trainable = False\n",
    "\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy', # using the cross-entropy loss function\n",
    "              optimizer='rmsprop', \n",
    "              metrics=['accuracy']) # reporting the accuracy\n",
    "[(l, \"Trainable: {}\".format(l.trainable)) for l in model.layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1603 samples, validate on 17 samples\n",
      "Epoch 1/30\n",
      "1603/1603 [==============================] - 8s 5ms/step - loss: 1.2333 - acc: 0.5839 - val_loss: 0.9830 - val_acc: 0.5294\n",
      "Epoch 2/30\n",
      "1603/1603 [==============================] - 8s 5ms/step - loss: 1.2003 - acc: 0.5602 - val_loss: 0.9425 - val_acc: 0.6471\n",
      "Epoch 3/30\n",
      "1603/1603 [==============================] - 8s 5ms/step - loss: 1.1753 - acc: 0.5870 - val_loss: 0.9692 - val_acc: 0.4706\n",
      "Epoch 4/30\n",
      "1603/1603 [==============================] - 8s 5ms/step - loss: 1.1426 - acc: 0.6014 - val_loss: 0.8852 - val_acc: 0.6471\n",
      "Epoch 5/30\n",
      "1603/1603 [==============================] - 7s 5ms/step - loss: 1.1173 - acc: 0.6369 - val_loss: 0.8369 - val_acc: 0.7059\n",
      "Epoch 6/30\n",
      "1603/1603 [==============================] - 7s 5ms/step - loss: 1.0754 - acc: 0.6625 - val_loss: 0.8338 - val_acc: 0.7647\n",
      "Epoch 7/30\n",
      "1603/1603 [==============================] - 7s 5ms/step - loss: 1.0464 - acc: 0.6943 - val_loss: 0.8745 - val_acc: 0.7059\n",
      "Epoch 8/30\n",
      "1603/1603 [==============================] - 8s 5ms/step - loss: 0.9910 - acc: 0.7062 - val_loss: 0.7895 - val_acc: 0.7647\n",
      "Epoch 9/30\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train, epochs=30, \n",
    "    batch_size=64, class_weight={0:1, 1:6}, validation_split=0.01,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metrics(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import basename\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras import backend as K\n",
    "\n",
    "def fix_layers(model, fixed_layers):\n",
    "    for i in range(fixed_layers):\n",
    "        model.layers[i].trainable = False\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop', \n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "def fine_tune(fixed_layers):\n",
    "    model = load_model(\"models/model.h5\")\n",
    "    \n",
    "    fix_layers(model, fixed_layers)\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_train, epochs=10, \n",
    "        batch_size=64, class_weight={0:1, 1:6}, validation_split=0.01,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_analysis(filename, fixed_layers=4):\n",
    "    K.clear_session()\n",
    "    model = load_model(\"models/model.h5\")\n",
    "    X, y = load_data(files[143])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y)\n",
    "    \n",
    "    ret = {\"file\" : basename(filename)}\n",
    "    metrics = {\"ft_0_{}\".format(k):v for k,v in get_metrics(model, X_test, y_test).items()}\n",
    "    ret.update(metrics)\n",
    "    \n",
    "    model = fine_tune(4)\n",
    "    metrics = {\"ft_4_{}\".format(k):v for k,v in get_metrics(model, X_test, y_test).items()}\n",
    "    ret.update(metrics)\n",
    "    \n",
    "    model = fine_tune(5)\n",
    "    metrics = {\"ft_5_{}\".format(k):v for k,v in get_metrics(model, X_test, y_test).items()}\n",
    "    ret.update(metrics)\n",
    "    \n",
    "    K.clear_session()\n",
    "    return ret\n",
    "\n",
    "get_analysis(files[100], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = [get_analysis(file) for file in files[-10:]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
